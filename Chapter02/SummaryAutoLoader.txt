# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, max, lower, when, initcap

# Initialize Spark session for Customer Data Aggregation
spark = SparkSession.builder.appName("CustomerDataAggregation").getOrCreate()

# Define the schema based on the known structure of the CSV
schema = "customer_id INT, name STRING, age INT, email STRING, customer_status STRING"

# Path to the uploaded CSV file
path_to_csv = "/mnt/data/part-00000-tid3200334632332214470-9b4dec79-7e2e-495d-8657-3b5457ed3753-108-1-c000.csv"

# Set up Auto Loader for reading data
df = spark.readStream.format("cloudFiles").option("cloudFiles.format", "csv").option("path", path_to_csv).option("header", "true").schema(schema).load()

# Create a table using the DataFrame
df.writeStream.format("delta").option("checkpointLocation", "/mnt/data/checkpoints").table("customers")

# Load the Auto Loader table as a DataFrame and perform aggregation operations
customers_df = spark.table("customers")
aggregated_df = customers_df.groupBy("customer_status").agg(count("*").alias("total_customers"), avg("age").alias("average_age"))
aggregated_df.show()

# Find the oldest customer per status
oldest_customer_df = customers_df.groupBy("customer_status").agg(max("age").alias("max_age")).join(customers_df, ["customer_status"], "inner").select("customer_status", "name", "age").filter(col("age") == col("max_age"))
oldest_customer_df.show()

# Optionally, save the results for further analysis
aggregated_df.write.format("delta").mode("overwrite").save("/mnt/data/aggregated_customer_data")

# Initialize Spark session for Customer Data Transformation
spark = SparkSession.builder.appName("CustomerDataTransformation").getOrCreate()

# Auto Loader setup to ingest CSV data
customers_df = spark.readStream.format("cloudFiles").option("cloudFiles.format", "csv").option("path", path_to_csv).option("header", "true").load()

# Define a table using the DataFrame for easier SQL operations
customers_df.writeStream.format("delta").option("checkpointLocation", "/mnt/checkpoints").table("customers")

# Apply transformations to standardize email addresses, categorize into age groups, and capitalize names
customers_df = customers_df.withColumn("email", lower(col("email"))).withColumn("age_group", when(col("age") < 18, "Under 18").when((col("age") >= 18) & (col("age") <= 35), "18-35").when((col("age") > 35) & (col("age") <= 65), "36-65").otherwise("Above 65")).withColumn("name", initcap(col("name")))
customers_df.show()

# Save the transformed data
customers_df.write.format("delta").mode("overwrite").save("/mnt/data/transformed_customers")

# Set up the session and define the directory path for incremental loading
path_to_csv_dir = "/mnt/data/csv_files/"

# Configure Auto Loader for incremental loading
customers_df = spark.readStream.format("cloudFiles").option("cloudFiles.format", "csv").option("cloudFiles.useIncrementalListing", "true").option("path", path_to_csv_dir).option("header", "true").schema(schema).load()

# Define a table for easier SQL operations and apply transformations
customers_df.writeStream.format("delta").option("checkpointLocation", "/mnt/checkpoints/customers").table("customers")
transformed_df = customers_df.withColumn("email", lower(col("email"))).withColumn("age_group", when(col("age") < 18, "Under 18").when((col("age") >= 18) & (col("age") <= 35), "18-35").when((col("age") > 35) & (col("age") <= 65), "36-65").otherwise("Above 65")).withColumn("name", initcap(col("name")))

# Writing the transformed data to a Delta table continuously
query = transformed_df.writeStream.format("delta").option("checkpointLocation", "/mnt/checkpoints/transformed_customers").outputMode("append").start("/mnt/data/transformed_customers")
query.awaitTermination()
